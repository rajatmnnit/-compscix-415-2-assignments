---
title: "COMPSCIX 415.2 Homework 5/Midterm"
author: "Rajat Jain"
date: "July 10th, 2018"
output:
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, echo=FALSE, warning=FALSE, message=FALSE}
# Load tidyverse library.
library(tidyverse)
```

# Exercises (Total Points - 30)


## My Git Repository

My git repository is located at this Github location: [https://github.com/rajatmnnit/compscix-415-2-assignments](https://github.com/rajatmnnit/compscix-415-2-assignments)

All the assignments are organized under their individual directories containing rmarkdown (.Rmd) file(s) and the knitted pdf version along with all the required data files and generated output files.

Midterm assignment can be found under [midterm](https://github.com/rajatmnnit/compscix-415-2-assignments/tree/master/midterm) directory.


## RStudio and R Markdown (3 points)

1. **Use markdown headers in your document to clearly separate each midterm question and add a table of contents to your document.**

Each section in this document starts with a ## Heading 2 style section title.

You can use the table of contents on the first page to navigate to various sections directly.

Wherever relevant, hyperlinks are clickable.
\pagebreak


## The tidyverse packages (3 points)
**By now you’ve used at least five different packages from the tidyverse for plotting, data munging, reshaping data, importing/exporting data, and using tibbles (the `tibble` package is used for this without you even realizing it’s there).**
&nbsp;

1. **Can you name which package is associated with each task below?**

Here is the list of `tidyverse` packages:

* Plotting - `ggplot2`
* Data munging/wrangling - `dplyr`
* Reshaping (speading and gathering) data - `tidyr`
* Importing/exporting data - `readr`

&nbsp;

2. **Now can you name two functions that you’ve used from each package that you listed above for these tasks?**

Some of the functions we have used from each of these packages are:

* Plotting - `ggplot()`, `geom_point()`
* Data munging/wrangling - `filter()`, `summarise()`
* Reshaping data - `spread()`, `gather()`
* Importing/exporting data - `read_csv()`, `read_delim()`


## R Basics (1.5 points)

1. **Fix this code *with the fewest number of changes* possible so it works:**
```{r rbasics_1_q, eval=FALSE}
My_data.name___is.too00ooLong! <- c( 1 , 2   , 3 )
```

While the variable name is too long and doesn't follow any conventions, it is not invalid to use it. The only problem is the ! mark in the variable name. Just removing it is the smallest change which makes this code work. 

```{r rbasics_1}
My_data.name___is.too00ooLong <- c( 1 , 2   , 3 )
```
&nbsp;

2. **Fix this code so it works:**
```{r rbasics_2_q, eval=FALSE}
my_string <- C('has', 'an', 'error', 'in', 'it)
```

Lower case the concat function `c()` and complete quote on the last value to fix this code.

```{r rbasics_2}
my_string <- c('has', 'an', 'error', 'in', 'it')
```
&nbsp;

3. **Look at the code below and comment on what happened to the values in the vector.**
```{r rbasics_3_q}
my_vector <- c(1, 2, '3', '4', 5)
my_vector
```

In R the vectors can contain values of only a single data type. Since we are trying to construct a vector with mixed values of numbers and characters (quoted numbers), R automatically converts everything to character. Hence all values are returned as characters on printing `my_vector`.
\pagebreak


## Data import/export (3 points)

1. **Download the rail_trail.txt file from Canvas (in the Midterm Exam section) and successfully import it into R. Prove that it was imported successfully by including your import code and taking a `glimpse` of the result.**

```{r dataio_1}
# File has been downloaded from Canvas to the midterm directory.
rail_trail <- read_delim("rail_trail.txt", delim = "|")

glimpse(rail_trail)
```
\pagebreak

2. **Export the file into a comma-separated file and name it “rail_trail.csv”. Make sure you define the `path` correctly so that you know where it gets saved. Then reload the file. Include your export and import code and take another `glimpse`.**

```{r dataio_2}
# Write the new CSV file locally to the midterm directory.
write_csv(rail_trail, path = "rail_trail.csv")

# Read back from midterm directory.
rail_trail_csv <- read_csv("rail_trail.csv")

glimpse(rail_trail_csv)
```
\pagebreak


## Visualization (6 points)

1. **Critique this graphic: give only three examples of what is wrong with this graphic. Be concise.**

There are a few features of this visualization which I disagree with.

a. **Area based Visualization** - I do not generally agree with bubble charts because they are area based representations of values which makes them difficult to interpret and compare.
b. **Multiple Dimensions on same axis** - It seems like the author represented multiple dimensions - Age Group and Sex on the same y-axis in the graphic. It seems confusing and definitely a bad idea.
c. **Percentages don't add up** - The sum of percentages between "Yes" and "No" responses on each row do not add up to 100%. In fact they are substantially lower than 100%. I believe there was another category in the survey form - "Don't Know" or equivalent `NA`. If this response is not important, I would probably remove NAs before computing percentages.

&nbsp;

2. **Reproduce this graphic using the `diamonds` data set.**

Here is the code which reproduces the plot from the question:

```{r visual_2}
ggplot(data = diamonds, aes(x = cut, y = carat, fill = color)) +
  geom_boxplot(position = "identity") +
  xlab("CUT OF DIAMOND") +
  ylab("CARAT OF DIAMOND") +
  coord_flip()
```
\pagebreak

3. **The previous graphic is not very useful. We can make it much more useful by changing one thing about it. Make the change and plot it again.**

The previous graphic is not very useful because individual color boxplots overlap each other which makes is unreadable. One simple way to fix this is by using the `dodge` position adjustment. Here is the code which generates the fixed plot:

```{r visual_3, fig.height=7}
ggplot(data = diamonds, aes(x = cut, y = carat, fill = color)) +
  geom_boxplot(position = "dodge") +
  xlab("CUT OF DIAMOND") +
  ylab("CARAT OF DIAMOND") +
  coord_flip()
```
\pagebreak


## Data munging and wrangling (6 points)

1. **Is this data “tidy”? If yes, leave it alone and go to the next problem. If no, make it tidy. *Note: this data set is called table2 and is available in the tidyverse package. It should be ready for you to use after you’ve loaded the tidyverse package.* **

```{r datamung_1_q}
table2
```

No, this data is not tidy because observations for `country` and `year` variables are split over multiple rows for different values of `type` variable. A tidy data should have one column per variable and one row for each observation. Here is the code to make it tidy:

```{r datamung_1}
table2 %>% 
  spread(key = type, value = count)
```

&nbsp;

2. **Create a new column in the `diamonds` data set called `price_per_carat` that shows the price of each diamond per carat (hint: divide). Only show me the code, not the output.**

```{r datamung_2, results='hide'}
diamonds %>% 
  mutate(price_per_carat = price / carat)
```
\pagebreak

3. **For each `cut` of diamond in the `diamonds` data set, how many diamonds, and what proportion, have a price > 10000 and a carat < 1.5? There are several ways to get to an answer, but your solution must use the data wrangling verbs from the tidyverse in order to get credit.**

```{r datamung_3}
diamonds %>%
  group_by(cut) %>%
  summarise(
    count = sum(price > 10000 & carat < 1.5, na.rm = TRUE),
    proportion = mean(price > 10000 & carat < 1.5, na.rm = TRUE)
  )
```

* **Do the results make sense? Why?**

Yes the results make sense to some extent. As the `cut` gets better from Fair to Ideal, the proportion of expensive diamonds, even though with lesser weight, increases. This shows that the price of diamonds also depend on the cut in addition to its weight in carats.
&nbsp;

* **Do we need to be wary of any of these numbers? Why?**

Yes, we should be careful while making any inferences based on these numbers. Look at the box plot below of `cut` v/s `price`, we can clearly see that the points we are using lie in the area normally considered as outliers.

```{r datamung_3_b, fig.height=3.5}
ggplot(data = diamonds, aes(x = cut, y = price)) +
  geom_boxplot()
```

\pagebreak


## EDA (6 points)
**Take a look at the `txhousing` data set that is included with the `ggplot2` package and answer these questions:**

1. **During what time period is this data from?**

```{r eda_1}
# TODO: Be careful with the mutate below. It is working fine for this
# dataset, but min/max may break due to months being 1 or 2 digit vals.
txhousing %>%
  mutate(yr_mon = paste0(year, "-", month)) %>%
  summarize(
    from = min(yr_mon),
    to = max(yr_mon)
  )
```

The data in `txhousing` data set is from January 2000 to July 2015.

&nbsp;

2. **How many cities are represented?**

```{r eda_2}
txhousing %>% 
  select(city) %>% 
  n_distinct()
```
There are 46 cities represented in `txhousing` data set.

&nbsp;

3. **Which city, month and year had the highest number of sales?**

```{r eda_3}
txhousing %>% 
  arrange(desc(sales))
```

Houston, in the month of July of 2015 had the highest number of sales.

&nbsp;

4. **What kind of relationship do you think exists between the number of listings and the number of sales? Check your assumption and show your work.**

I think the number of listings should have a strong positive correlation with the number of sales. The more the listings, the more chances of making a sale. 

```{r eda_4, message=FALSE, warning=FALSE, fig.height=3.5}
ggplot(data = txhousing, aes(x = listings, y = sales)) +
  geom_point(alpha = 0.4, color = "red") + 
  geom_smooth()
```

The scatter plot above confirms my assumption.

&nbsp;

5. **What proportion of `sales` is missing for each city?**

Here is a list of proportion of `sales` missing by each `city` sorted in descending order of missing proportion.
```{r eda_5}
txhousing %>%
  group_by(city) %>%
  summarize(prop_missing = mean(is.na(sales))) %>%
  arrange(desc(prop_missing))
```
\pagebreak

6. **Looking at only the cities and months with greater than 500 sales:**

* **Are the distributions of the median sales price (column name `median`), when grouped by city, different? The same? Show your work.**

```{r eda_6_a, message=FALSE, fig.height=4.5}
txhousing %>%
  filter(sales > 500) %>%
  ggplot(aes(x = reorder(city, median, FUN = mean), y = median, fill = city)) + 
  geom_boxplot(show.legend = FALSE) + 
  xlab("City") + ylab("Median Sales Price") + 
  coord_flip()
```

The distribution of median sales prices certainly differs by city. Some cities like "Colin Country" have higher median sales price compared to others. The distribution around median varies a lot as well. For example, cities like "El Paso", "San Antonio", "Fort Bend" etc. have higher variance around median, whereas cities like "Corpus Christi" and "Arlington" have very less variance.

&nbsp;

* **Any cities that stand out that you’d want to investigate further?**

Yes. We should further investigate the data for the cities of "Dallas", "Colin County" and "Houston" because they seem to have too many outliers.

&nbsp;

* **Why might we want to filter out all cities and months with sales less than 500?**

We want to filter out all cities and months with sales less than 500 because having them all on our plot will make it unreadable. Also these observations will not be useful in the EDA as they represent the sparce sales and not the majority of sales.
\pagebreak
    
    
## Git and Github (1.5 points)
**To demonstrate your use of git and Github, at the top of your document put a hyperlink to your Github repository.**

**Once you are finished with your midterm, commit your final changes with the comment “finished the midterm - woohoo” and push your R Markdown file and your html or pdf file to Github.**

See the Section [My Git Repository](#my-git-repository) above.